{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(for github) lstm-opimazation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJzpdnYWc0hM"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGSnuoP3x0aj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7de772e-9303-4f88-fe19-922428aae989"
      },
      "source": [
        "# setup\n",
        "!pip install -q tensorflow-model-optimization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██                              | 10kB 32.9MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20kB 34.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 30kB 22.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 40kB 26.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 51kB 27.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 61kB 30.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 71kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 81kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 92kB 19.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 102kB 19.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 112kB 19.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 122kB 19.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 133kB 19.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 143kB 19.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 153kB 19.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 163kB 19.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174kB 19.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJejGZklayl5"
      },
      "source": [
        "### Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRAkKxXxutZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a5264ab-d12d-4901-f2c2-0fe27c37e511"
      },
      "source": [
        "import os\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# use tensorflow\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "# use tensorflow model optimization\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "# mount the Google Drive to Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "%cd \"/content/drive/My Drive/\"\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl27ocOFbR9g"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzVVhtg_xE10"
      },
      "source": [
        "f = open('accgry_segments.pckl', 'rb')\n",
        "segments = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "f = open('accgry_labels.pckl', 'rb')\n",
        "labels = pickle.load(f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7BmO7LkxPuS"
      },
      "source": [
        "# config based on data preprocessing\n",
        "sampling_freq = 100\n",
        "window_size = int(5.12*sampling_freq)\n",
        "overlap = 1*sampling_freq # 1s overlap\n",
        "feature_size = 18\n",
        "labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
        "reshaped_segments = segments.reshape(len(segments),window_size, feature_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWcP4eS9bc-q"
      },
      "source": [
        "### Split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCSyeFQtxWiE"
      },
      "source": [
        "train_x, test_x, train_y, test_y = train_test_split(reshaped_segments, labels, test_size=0.1, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIKgpJYHcP8d"
      },
      "source": [
        "### Load the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrYJdQhQx7Uw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb13c0c-462f-4baa-c181-ae2074a316c8"
      },
      "source": [
        "# load the trained lstm model\n",
        "lstm = tf.keras.models.load_model('lstm')\n",
        "# check the model architecture\n",
        "lstm.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_7 (LSTM)                (None, 512, 100)          47600     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512, 100)          0         \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 12)                1212      \n",
            "=================================================================\n",
            "Total params: 139,312\n",
            "Trainable params: 139,312\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4focTXy2TgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d7f97b8-7e76-401c-f442-ace8737f3744"
      },
      "source": [
        "# evaluate model\n",
        "_, lstm_accuracy = lstm.evaluate(test_x, test_y, verbose=0)\n",
        "lstm_accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9826689958572388"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF24lR0Ywl8i"
      },
      "source": [
        "## Fine-tune pre-trained model with pruning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-TLW57EdASP"
      },
      "source": [
        "### Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OOp9sRIwnss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e022d0-f5c3-4865-d59f-6c5bd2590b4b"
      },
      "source": [
        "# start the model with 50% sparsity (50% zeros in weights) and end with 80% sparsity.\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "# compute end step to finish pruning after 10 epochs.\n",
        "batch_size = 64\n",
        "epochs = 15\n",
        "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
        "\n",
        "num_features = train_x.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_features / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "# define model for pruning.\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                               final_sparsity=0.80,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=end_step)\n",
        "}\n",
        "\n",
        "model_for_pruning = prune_low_magnitude(lstm, **pruning_params)\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_for_pruning.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:2281: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_lstm_7 ( (None, 512, 100)          94803     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 512, 100)          1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_lstm_8 ( (None, 100)               160403    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_8  (None, 100)               20102     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_9  (None, 12)                2414      \n",
            "=================================================================\n",
            "Total params: 277,723\n",
            "Trainable params: 139,312\n",
            "Non-trainable params: 138,411\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AratpDuvyhJg"
      },
      "source": [
        "### Prun the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvCWQ6ENyhlX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aed232f8-9e67-4d91-c2da-a36ad99ded6f"
      },
      "source": [
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "  keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "]\n",
        "  \n",
        "model_for_pruning.fit(train_x, train_y,\n",
        "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
        "                  callbacks=callbacks)\n",
        "\n",
        "model_for_pruning = tf.keras.models.load_model('lstm_pruning')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "  6/219 [..............................] - ETA: 34s - loss: 0.2095 - accuracy: 0.9288WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0674s vs `on_train_batch_end` time: 0.0770s). Check your callbacks.\n",
            "219/219 [==============================] - 19s 68ms/step - loss: 0.1060 - accuracy: 0.9681 - val_loss: 0.1352 - val_accuracy: 0.9602\n",
            "Epoch 2/15\n",
            "219/219 [==============================] - 13s 61ms/step - loss: 0.0850 - accuracy: 0.9775 - val_loss: 0.1009 - val_accuracy: 0.9724\n",
            "Epoch 3/15\n",
            "219/219 [==============================] - 13s 62ms/step - loss: 0.0706 - accuracy: 0.9786 - val_loss: 0.1202 - val_accuracy: 0.9641\n",
            "Epoch 4/15\n",
            "219/219 [==============================] - 13s 61ms/step - loss: 0.0743 - accuracy: 0.9772 - val_loss: 0.0937 - val_accuracy: 0.9705\n",
            "Epoch 5/15\n",
            "219/219 [==============================] - 13s 62ms/step - loss: 0.0725 - accuracy: 0.9797 - val_loss: 0.0793 - val_accuracy: 0.9750\n",
            "Epoch 6/15\n",
            "219/219 [==============================] - 13s 60ms/step - loss: 0.0789 - accuracy: 0.9757 - val_loss: 0.0949 - val_accuracy: 0.9673\n",
            "Epoch 7/15\n",
            "219/219 [==============================] - 14s 64ms/step - loss: 0.0554 - accuracy: 0.9830 - val_loss: 0.1080 - val_accuracy: 0.9634\n",
            "Epoch 8/15\n",
            "219/219 [==============================] - 13s 61ms/step - loss: 0.0620 - accuracy: 0.9812 - val_loss: 0.0915 - val_accuracy: 0.9718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17GgVhLXfBuE"
      },
      "source": [
        "### Summarize the pruned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsNfHrAIhkp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b99fd8-87d0-424e-964e-c67718ef04a3"
      },
      "source": [
        "# summarize the pruned model architecture\n",
        "model_for_pruning.summary()\n",
        "\n",
        "# evaluate the pruned model \n",
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(test_x, test_y, verbose=0)\n",
        "\n",
        "# compare the pruned model with baseline\n",
        "print('Baseline test accuracy:',lstm_accuracy) \n",
        "print('Pruned test accuracy:', model_for_pruning_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_lstm_7 ( (None, 512, 100)          87602     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 512, 100)          1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_lstm_8 ( (None, 100)               120402    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_8  (None, 100)               20102     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_9  (None, 12)                2414      \n",
            "=================================================================\n",
            "Total params: 230,521\n",
            "Trainable params: 139,312\n",
            "Non-trainable params: 91,209\n",
            "_________________________________________________________________\n",
            "Baseline test accuracy: 0.9826689958572388\n",
            "Pruned test accuracy: 0.9809358716011047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M286Ib9jfmry"
      },
      "source": [
        "### Create 3x smaller models from pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY8R3uLW8pU4"
      },
      "source": [
        "def get_gzipped_model_size(file):\n",
        "  # returns size of gzipped model, in bytes.\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C1U1VSc9Y77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9f15bbf6-e878-410b-e7fb-11fd25d57551"
      },
      "source": [
        "# first, create a compressible model for TensorFlow\n",
        "_, keras_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(lstm, keras_file, include_optimizer=False)\n",
        "print('Saved baseline model to:', keras_file)\n",
        "\n",
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved baseline model to: /tmp/tmpab52pzt3.h5\n",
            "Saved pruned Keras model to: /tmp/tmprdtauiq_.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGOxTG6N83Ym",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3fba443d-3935-44e5-d4f6-0c2ffe5b901d"
      },
      "source": [
        "# apply a standard compression algorithm (e.g. via gzip)\n",
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of gzipped baseline Keras model: 520224.00 bytes\n",
            "Size of gzipped pruned Keras model: 193012.00 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}